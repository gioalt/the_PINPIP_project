# The_PINPIP_project

the Pinterest Data Pipeline project (no 3)

# Abstract

The purpose of this project is to build two types of data engineering pipelines for processing *batch* (periodical, complete, high-volume, repetitive) and *streaming* (continuous,high-volume,incremental) data, from the Pinterest social media service. 

Distributed computing services (AWS Cloud Computing) are employed; data sets are stored in predisposed cloud S3 buckets as well as in tables. An outline of the basic components of these ETL pipelines is as follows:

*Batch data*  
    1. Amazon API Gateway  
    2. Virtual Private Cloud: EC2 Instance  
        2a. Confluent-kafka-connect-s3  
        2b. Confluent kafka REST proxy  
        2c. Kafka consumer  
    3. Amazon S3  
    4. Databricks Notebook
    5. Airflow DAG, daily schedule 

*Streaming data*  
    1. Amazon API Gateway  
    2. Virtual Private Cloud: Amazon Kinesis  
    3. Databricks Delta Lake 

Data is generated by a Python emulator code, which mimics the actual pinterest data formats. The data sets are encoded in json format, each readable with the json_loads(<string that point to name>) method into a python dictionary. A glimpse of the three types of data is here below:

        pin_data
	        *post* {'index': 7528,'unique_id': 'fe34b356 xxxx' , 'title': 'No title', (description,poster_name,follower_count, tag_list,is_image_or_video,image_src,downloaded,save_location,category)}
	
        geolocation_data
            *geo* {'ind': 7528, 'timestamp': xxx ,'latitude': ,'longitude': , 'country': }
    
        user_data
            *user* {'ind': 7528, 'first_name':'Abigail', 'last_name':'Ali','age':'25','date_joined: cxxc}
    
In summary, there are 12 key-value pairs in *post*; 5 key-value in *geo*; 5 key-value in *usr*. Their *primary keys* are index/ind, which seem the same variable but with different name. Noticeably, the time formats require pre-processing to conform to API time ISO 8601 standard. Processed data exhibit different keys, according to milestones/tasks' presciption. 

# Table of Content

emulators:   

        user_posting_emulation_*{pin,geo,user}*.py (batch)  
        user_posting_emulation_stream_*{pin,geo,user}*.py  

notebooks:  

        New_Extract_Transform_BatchData.ipynb   
        New_Extract_Transform_StreamData.ipynb   

# Description of the project

Two types of ETL pipelines are assembled for processing batch and streaming data using Amazon Web Services. Each pipeline is constituted by two types of files: emulators and notebooks.

The emulators (.py file extension) are pieces of code aimed at simulating the 'scraping' of Pinterest data, which produce batches and/or streaming of different data sets. The notebooks (.ipynb file extension) are pieces of code which process the 'scraped' data.


*Compute specification of Pinterest Cluster*. Driver: m4.large · Workers: m4.large · 1-8 workers · On-demand and Spot · fall back to On-demand · 10.4 LTS (includes Apache Spark 3.2.1, Scala 2.12) · auto


# Usage Instructions

Emulators run before Notebooks.

# Licence Information

The PINPIP suite was built in partial fullfilment of the requirements for the AICore Data Engineering certification. This program suite is not covered by licence, but this project cannot be used without appropriate credentials. 
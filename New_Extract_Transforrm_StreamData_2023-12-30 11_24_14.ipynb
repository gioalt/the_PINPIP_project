{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f94687-d043-4f20-af39-3d2d2787cd51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read streaming data from Kinesis\n",
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e309b2eb-aff1-4077-96b4-467dfa51350a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "#ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access #key ID').collect()[0]['Access key ID']\n",
    "#SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "#ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "# new version\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc5142b-d39a-4b85-a721-0854029a7b7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Capture pin data stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01474a2c-d1f5-4e6f-b55d-fcbfe2d5cd58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pin_stream = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0ea9a6e05a33-pin') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "pin_schema = StructType([\n",
    "StructField(\"index\", StringType()),\n",
    "StructField(\"unique_id\", StringType()),\n",
    "StructField(\"title\", StringType()),\n",
    "StructField(\"description\", StringType()),\n",
    "StructField(\"follower_count\", StringType()),\n",
    "StructField(\"poster_name\", StringType()),\n",
    "StructField(\"tag_list\", StringType()),\n",
    "StructField(\"is_image_or_video\", StringType()),\n",
    "StructField(\"image_src\", StringType()),\n",
    "StructField(\"downloaded\", StringType()),\n",
    "StructField(\"save_location\", StringType()),\n",
    "StructField(\"category\", StringType())\n",
    "])\n",
    "\n",
    "df_pin = df_pin_stream \\\n",
    ".selectExpr(\"CAST(data as STRING)\") \\\n",
    ".select(from_json(col(\"data\"), pin_schema).alias(\"pin_data\")) \\\n",
    ".select(\"pin_data.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6808786-ef5c-4bb3-9421-7aaad79bae3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### clean pin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224b52b2-ea0b-4919-924d-aacf72fa57b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-2998023767729582&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     23</span> df_pin_cleaned<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;ind&#34;</span><span class=\"ansi-blue-fg\">,</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;ind&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>cast<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;int&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     24</span> \n",
       "<span class=\"ansi-green-fg\">---&gt; 25</span><span class=\"ansi-red-fg\"> </span>df_pin_cleaned<span class=\"ansi-blue-fg\">.</span>writeStream<span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     26</span> <span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;delta&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     27</span> <span class=\"ansi-blue-fg\">.</span>outputMode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;append&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/streaming.py</span> in <span class=\"ansi-cyan-fg\">table</span><span class=\"ansi-blue-fg\">(self, tableName)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1231</span>         &#34;&#34;&#34;Alias for the toTable API\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1232</span>         &#34;&#34;&#34;\n",
       "<span class=\"ansi-green-fg\">-&gt; 1233</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>toTable<span class=\"ansi-blue-fg\">(</span>tableName<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1234</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1235</span>     def toTable(self, tableName, format=None, outputMode=None, partitionBy=None, queryName=None,\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/streaming.py</span> in <span class=\"ansi-cyan-fg\">toTable</span><span class=\"ansi-blue-fg\">(self, tableName, format, outputMode, partitionBy, queryName, **options)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1301</span>         <span class=\"ansi-green-fg\">if</span> queryName <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span>             self<span class=\"ansi-blue-fg\">.</span>queryName<span class=\"ansi-blue-fg\">(</span>queryName<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1303</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_sq<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>toTable<span class=\"ansi-blue-fg\">(</span>tableName<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1305</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n",
       "\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o5180.toTable.\n",
       ": java.util.concurrent.TimeoutException: Stream Execution thread for stream [id = 0339b0ac-0973-403e-bebc-d3826ee7c8ef, runId = ea501f88-7abe-4ad7-9d3a-74373139b4d9] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.interruptAndAwaitExecutionThreadTermination(StreamExecution.scala:560)\n",
       "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.stop(MicroBatchExecution.scala:216)\n",
       "\tat org.apache.spark.sql.streaming.StreamingQueryManager.$anonfun$startQuery$7(StreamingQueryManager.scala:416)\n",
       "\tat org.apache.spark.sql.streaming.StreamingQueryManager.$anonfun$startQuery$7$adapted(StreamingQueryManager.scala:414)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:414)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:459)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:438)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:254)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.writeToV1Table$1(DataStreamWriter.scala:348)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.toTable(DataStreamWriter.scala:361)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:295)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: The stream thread was last executing:\n",
       "\tat java.lang.Object.wait(Native Method)\n",
       "\tat java.lang.Thread.join(Thread.java:1257)\n",
       "\tat java.lang.Thread.join(Thread.java:1331)\n",
       "\tat com.databricks.sql.kinesis.KinesisMicroBatchStream.stop(KinesisMicroBatchStream.scala:366)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1(StreamExecution.scala:515)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1$adapted(StreamExecution.scala:513)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$12824/440977070.apply(Unknown Source)\n",
       "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.stopSources(StreamExecution.scala:513)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$4(StreamExecution.scala:407)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$12823/1969092281.apply(Unknown Source)\n",
       "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:396)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$12445/427462288.apply$mcV$sp(Unknown Source)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$$$Lambda$284/1275012287.apply(Unknown Source)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:22)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:63)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:163)\n",
       "\tat com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:493)\n",
       "\tat com.databricks.spark.util.UsageLogging$$Lambda$12446/491063655.apply(Unknown Source)\n",
       "\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:605)\n",
       "\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:614)\n",
       "\tat com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:493)\n",
       "\tat com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:491)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.withAttributionTags(StreamExecution.scala:78)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:314)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:255)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2998023767729582&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     23</span> df_pin_cleaned<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;ind&#34;</span><span class=\"ansi-blue-fg\">,</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;ind&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>cast<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;int&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     24</span> \n<span class=\"ansi-green-fg\">---&gt; 25</span><span class=\"ansi-red-fg\"> </span>df_pin_cleaned<span class=\"ansi-blue-fg\">.</span>writeStream<span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     26</span> <span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;delta&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     27</span> <span class=\"ansi-blue-fg\">.</span>outputMode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;append&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/streaming.py</span> in <span class=\"ansi-cyan-fg\">table</span><span class=\"ansi-blue-fg\">(self, tableName)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1231</span>         &#34;&#34;&#34;Alias for the toTable API\n<span class=\"ansi-green-intense-fg ansi-bold\">   1232</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1233</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>toTable<span class=\"ansi-blue-fg\">(</span>tableName<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1234</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1235</span>     def toTable(self, tableName, format=None, outputMode=None, partitionBy=None, queryName=None,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/streaming.py</span> in <span class=\"ansi-cyan-fg\">toTable</span><span class=\"ansi-blue-fg\">(self, tableName, format, outputMode, partitionBy, queryName, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1301</span>         <span class=\"ansi-green-fg\">if</span> queryName <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span>             self<span class=\"ansi-blue-fg\">.</span>queryName<span class=\"ansi-blue-fg\">(</span>queryName<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1303</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_sq<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>toTable<span class=\"ansi-blue-fg\">(</span>tableName<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1305</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o5180.toTable.\n: java.util.concurrent.TimeoutException: Stream Execution thread for stream [id = 0339b0ac-0973-403e-bebc-d3826ee7c8ef, runId = ea501f88-7abe-4ad7-9d3a-74373139b4d9] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.interruptAndAwaitExecutionThreadTermination(StreamExecution.scala:560)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.stop(MicroBatchExecution.scala:216)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.$anonfun$startQuery$7(StreamingQueryManager.scala:416)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.$anonfun$startQuery$7$adapted(StreamingQueryManager.scala:414)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:414)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:459)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:438)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:254)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.writeToV1Table$1(DataStreamWriter.scala:348)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.toTable(DataStreamWriter.scala:361)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: The stream thread was last executing:\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1257)\n\tat java.lang.Thread.join(Thread.java:1331)\n\tat com.databricks.sql.kinesis.KinesisMicroBatchStream.stop(KinesisMicroBatchStream.scala:366)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1(StreamExecution.scala:515)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1$adapted(StreamExecution.scala:513)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$12824/440977070.apply(Unknown Source)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.stopSources(StreamExecution.scala:513)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$4(StreamExecution.scala:407)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$12823/1969092281.apply(Unknown Source)\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:396)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$12445/427462288.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat com.databricks.logging.Log4jUsageLoggingShim$$$Lambda$284/1275012287.apply(Unknown Source)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:22)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:63)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:163)\n\tat com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:493)\n\tat com.databricks.spark.util.UsageLogging$$Lambda$12446/491063655.apply(Unknown Source)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:605)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:614)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:493)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:491)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.withAttributionTags(StreamExecution.scala:78)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:314)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:255)\n</div>",
       "errorSummary": "java.util.concurrent.TimeoutException: Stream Execution thread for stream [id = 0339b0ac-0973-403e-bebc-d3826ee7c8ef, runId = ea501f88-7abe-4ad7-9d3a-74373139b4d9] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pin_cleaned = df_pin.withColumn(\"category\", when(df_pin[\"category\"].isNull(), None).otherwise(df_pin[\"category\"]))\n",
    "df_pin_cleaned = df_pin.withColumn(\"description\", when(df_pin[\"description\"].isNull(), None).otherwise(df_pin[\"description\"]))\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\n",
    "    \"follower_count\",\n",
    "    when(col(\"follower_count\").endswith(\"k\"), \n",
    "         regexp_replace(col(\"follower_count\"), \"k\", \"\").cast(\"int\") * 1000)\n",
    "    .when(col(\"follower_count\").endswith(\"M\"), \n",
    "          regexp_replace(col(\"follower_count\"), \"M\", \"\").cast(\"int\") * 1000000)\n",
    "    .otherwise(col(\"follower_count\").cast(\"int\"))\n",
    ")\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\n",
    "    \"follower_count\",\n",
    "    when(\n",
    "        df_pin_cleaned[\"follower_count\"].cast(\"int\").isNotNull(),\n",
    "        df_pin_cleaned[\"follower_count\"].cast(\"int\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "df_pin_cleaned.withColumn(\"downloaded\",col(\"downloaded\").cast(\"int\"))\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"save_location\",regexp_replace(col(\"save_location\"), \"^Local save in \", \"\"))\n",
    "df_pin_cleaned = df_pin_cleaned.withColumnRenamed(\"index\", \"ind\")\n",
    "desired_column_order = [\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"]\n",
    "df_pin_cleaned = df_pin_cleaned.select(desired_column_order)\n",
    "df_pin_cleaned.withColumn(\"ind\",col(\"ind\").cast(\"int\"))\n",
    "\n",
    "df_pin_cleaned.writeStream \\\n",
    ".format(\"delta\") \\\n",
    ".outputMode(\"append\") \\\n",
    ".option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    ".table(\"0ea9a6e05a33_pin_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42a055a-f79a-4c7b-86a1-55dc8a1c2c26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Capture geo data stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d60c06-78e2-4af0-86fd-05a6ea08caf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_geo_stream = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0ea9a6e05a33-geo') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "geo_schema = StructType([\n",
    "StructField(\"ind\", StringType()),\n",
    "StructField(\"timestamp\", StringType()),\n",
    "StructField(\"latitude\", DoubleType()),\n",
    "StructField(\"longitude\", DoubleType()),\n",
    "StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "df_geo = df_geo_stream \\\n",
    ".selectExpr(\"CAST(data as STRING)\") \\\n",
    ".select(from_json(col(\"data\"), geo_schema).alias(\"geo_data\")) \\\n",
    ".select(\"geo_data.*\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ad4cc70-d5a4-4618-9629-a603e8fc97f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### clean geo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b09c0017-2c1d-4b33-8558-abf8a9406730",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[32]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fabb1a281f0&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[32]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fabb1a281f0&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_geo_cleaned = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "df_geo_cleaned = df_geo_cleaned.drop(\"latitude\", \"longitude\")\n",
    "df_geo_cleaned = df_geo_cleaned.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "df_geo_cleaned = df_geo_cleaned.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "\n",
    "df_geo_cleaned.writeStream \\\n",
    ".format(\"delta\") \\\n",
    ".outputMode(\"append\") \\\n",
    ".option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    ".table(\"0ea9a6e05a33_geo_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f90eb66c-9a3c-40b1-8e9a-fdc250950e92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Capture user data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c7290f-ff41-4f42-8b86-6e0ed6f22e10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_user_streams = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0ea9a6e05a33-user') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "user_schema = StructType([\n",
    "    StructField(\"ind\", StringType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"date_joined\", TimestampType())\n",
    "])\n",
    "\n",
    "df_user = df_user_streams \\\n",
    "    .selectExpr(\"CAST(data as STRING)\") \\\n",
    "    .select(from_json(col(\"data\"), user_schema).alias(\"user_data\")) \\\n",
    "    .select(\"user_data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c5e863-3c31-460d-8f95-e4f57a61a800",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Clean user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d51bd9a3-f2c0-4812-83f6-6fbcb4d7f6a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[35]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fabb1a384c0&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[35]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fabb1a384c0&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_user_cleaned = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "df_user_cleaned = df_user_cleaned.drop(\"first_name\", \"last_name\")\n",
    "df_user_cleaned = df_user_cleaned.withColumn(\"date_joined\", to_timestamp(col(\"date_joined\"), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "df_user_cleaned = df_user_cleaned.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "     \n",
    "df_user_cleaned.writeStream \\\n",
    ".format(\"delta\") \\\n",
    ".outputMode(\"append\") \\\n",
    ".option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    ".table(\"0ea9a6e05a33_user_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "New_Extract_Transforrm_StreamData_2023-12-30 11:24:14",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
